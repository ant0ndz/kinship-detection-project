{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73aa42d-4e2e-4e45-a9a4-5ace6e21808f",
   "metadata": {},
   "source": [
    "# Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b5f4bf8-b074-4050-9787-1a609d919dbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "deepface_env          *  /home/dzega/.conda/envs/deepface_env\n",
      "env_full                 /home/dzega/.conda/envs/env_full\n",
      "face_recognition_env     /home/dzega/.conda/envs/face_recognition_env\n",
      "jupyterlab               /storage/modules/envs/jupyterlab\n",
      "base                     /storage/modules/packages/anaconda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62398e31-0693-40c0-b09d-0237349b931d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/dzega/.conda/envs/deepface_env:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                        main    defaults\n",
      "_openmp_mutex             5.1                       1_gnu    defaults\n",
      "asttokens                 3.0.0                    pypi_0    pypi\n",
      "ca-certificates           2024.12.31           h06a4308_0    defaults\n",
      "certifi                   2025.1.31                pypi_0    pypi\n",
      "charset-normalizer        3.4.1                    pypi_0    pypi\n",
      "comm                      0.2.2                    pypi_0    pypi\n",
      "debugpy                   1.8.12                   pypi_0    pypi\n",
      "decorator                 5.1.1                    pypi_0    pypi\n",
      "exceptiongroup            1.2.2                    pypi_0    pypi\n",
      "executing                 2.2.0                    pypi_0    pypi\n",
      "facenet-pytorch           2.6.0                    pypi_0    pypi\n",
      "filelock                  3.13.1                   pypi_0    pypi\n",
      "fsspec                    2024.6.1                 pypi_0    pypi\n",
      "idna                      3.10                     pypi_0    pypi\n",
      "importlib-metadata        8.6.1                    pypi_0    pypi\n",
      "ipykernel                 6.29.5                   pypi_0    pypi\n",
      "ipython                   8.18.1                   pypi_0    pypi\n",
      "jedi                      0.19.2                   pypi_0    pypi\n",
      "jinja2                    3.1.4                    pypi_0    pypi\n",
      "joblib                    1.4.2                    pypi_0    pypi\n",
      "jupyter-client            8.6.3                    pypi_0    pypi\n",
      "jupyter-core              5.7.2                    pypi_0    pypi\n",
      "ld_impl_linux-64          2.40                 h12ee557_0    defaults\n",
      "libffi                    3.4.4                h6a678d5_1    defaults\n",
      "libgcc-ng                 11.2.0               h1234567_1    defaults\n",
      "libgomp                   11.2.0               h1234567_1    defaults\n",
      "libstdcxx-ng              11.2.0               h1234567_1    defaults\n",
      "markupsafe                2.1.5                    pypi_0    pypi\n",
      "matplotlib-inline         0.1.7                    pypi_0    pypi\n",
      "mpmath                    1.3.0                    pypi_0    pypi\n",
      "ncurses                   6.4                  h6a678d5_0    defaults\n",
      "nest-asyncio              1.6.0                    pypi_0    pypi\n",
      "networkx                  3.2.1                    pypi_0    pypi\n",
      "numpy                     1.26.4                   pypi_0    pypi\n",
      "nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\n",
      "nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\n",
      "nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\n",
      "nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\n",
      "nvidia-cudnn-cu12         8.9.2.26                 pypi_0    pypi\n",
      "nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\n",
      "nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\n",
      "nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\n",
      "nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\n",
      "nvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\n",
      "nvidia-nccl-cu12          2.19.3                   pypi_0    pypi\n",
      "nvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\n",
      "nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\n",
      "openssl                   3.0.15               h5eee18b_0    defaults\n",
      "packaging                 24.2                     pypi_0    pypi\n",
      "pandas                    2.2.3                    pypi_0    pypi\n",
      "parso                     0.8.4                    pypi_0    pypi\n",
      "pexpect                   4.9.0                    pypi_0    pypi\n",
      "pillow                    10.2.0                   pypi_0    pypi\n",
      "pip                       25.0             py39h06a4308_0    defaults\n",
      "platformdirs              4.3.6                    pypi_0    pypi\n",
      "prompt-toolkit            3.0.50                   pypi_0    pypi\n",
      "psutil                    6.1.1                    pypi_0    pypi\n",
      "ptyprocess                0.7.0                    pypi_0    pypi\n",
      "pure-eval                 0.2.3                    pypi_0    pypi\n",
      "pygments                  2.19.1                   pypi_0    pypi\n",
      "python                    3.9.21               he870216_1    defaults\n",
      "python-dateutil           2.9.0.post0              pypi_0    pypi\n",
      "pytz                      2025.1                   pypi_0    pypi\n",
      "pyzmq                     26.2.1                   pypi_0    pypi\n",
      "readline                  8.2                  h5eee18b_0    defaults\n",
      "requests                  2.32.3                   pypi_0    pypi\n",
      "scikit-learn              1.6.1                    pypi_0    pypi\n",
      "scipy                     1.13.1                   pypi_0    pypi\n",
      "setuptools                75.8.0           py39h06a4308_0    defaults\n",
      "six                       1.17.0                   pypi_0    pypi\n",
      "sqlite                    3.45.3               h5eee18b_0    defaults\n",
      "stack-data                0.6.3                    pypi_0    pypi\n",
      "sympy                     1.13.1                   pypi_0    pypi\n",
      "threadpoolctl             3.5.0                    pypi_0    pypi\n",
      "tk                        8.6.14               h39e8969_0    defaults\n",
      "torch                     2.2.2                    pypi_0    pypi\n",
      "torchaudio                2.6.0+cu124              pypi_0    pypi\n",
      "torchvision               0.17.2                   pypi_0    pypi\n",
      "tornado                   6.4.2                    pypi_0    pypi\n",
      "tqdm                      4.67.1                   pypi_0    pypi\n",
      "traitlets                 5.14.3                   pypi_0    pypi\n",
      "triton                    2.2.0                    pypi_0    pypi\n",
      "typing-extensions         4.12.2                   pypi_0    pypi\n",
      "tzdata                    2025.1                   pypi_0    pypi\n",
      "urllib3                   2.3.0                    pypi_0    pypi\n",
      "wcwidth                   0.2.13                   pypi_0    pypi\n",
      "wheel                     0.45.1           py39h06a4308_0    defaults\n",
      "xz                        5.4.6                h5eee18b_1    defaults\n",
      "zipp                      3.21.0                   pypi_0    pypi\n",
      "zlib                      1.2.13               h5eee18b_1    defaults\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf75a1ad-e97f-4384-b44d-7494d1bffd8f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b35838-63c8-4aaa-80c7-576c147d6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kinship_image_data_class import *\n",
    "\n",
    "# from kinship_image_data_class import KinshipPairs\n",
    "# from kinship_image_data_class import KinshipDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed519a23-b154-4533-a84c-6a39233989dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzega/.conda/envs/deepface_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Common\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Files\n",
    "from pathlib import Path\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Machine Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import VGG16_Weights, VGG19_Weights, ResNet50_Weights, ResNet152_Weights\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Face Net\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "\n",
    "# Other\n",
    "from typing import List, Tuple, Optional\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d080911f-28bc-40c2-a297-39a38abb942d",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcbb9be-4170-4ccf-81a3-337255e0f9ee",
   "metadata": {},
   "source": [
    "## Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e381c0a7-1a6a-4b2a-8d9e-f3fe17f5447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior in cuDNN\n",
    "    torch.backends.cudnn.benchmark = False  # Disables auto-tuning for deterministic behavior\n",
    "\n",
    "SEED = 42\n",
    "set_seed(seed=SEED)\n",
    "\n",
    "# Note: This seed does not make the machin learning proccess and results deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738680cd-57f4-4ded-86c1-c16fc2ea262f",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3210db4-3220-4224-8285-2428fa8ecf8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the current working directory (cwd)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Get the grandparent directory\n",
    "grandparent_dir = os.path.abspath(os.path.join(current_dir, \"..\", \"..\"))\n",
    "\n",
    "# Paths to data files\n",
    "train_pairs_dir = os.path.join(grandparent_dir, \"Data/families_in_the_wild/train/train-relationship-lists\")\n",
    "train_families_dir = os.path.join(grandparent_dir, \"Data/families_in_the_wild/train/train-faces\")\n",
    "\n",
    "test_relationship_lists_dir = os.path.join(grandparent_dir, \"Data/families_in_the_wild/test/test-relationship-lists\")\n",
    "test_faces_dir = os.path.join(grandparent_dir, \"Data/families_in_the_wild/test/test-faces\")\n",
    "test_relationship_labels_dir = os.path.join(grandparent_dir, \"Data/families_in_the_wild/test/test-relationship-labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a2d61d4-cf64-40a1-ba11-016699b6923a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############ Created Datasets with the following characteristics: ############\n",
      "Balanced classes: True\n",
      "Train ratio: 0.8\n",
      "Validation ratio: 0.1\n",
      "Test ratio: 0.1\n",
      "#### Families ####\n",
      "Number of training families: 565\n",
      "Number of validation families: 62\n",
      "Number of test families: 62\n",
      "#### Set Pairs ####\n",
      "\n",
      "Train Statistics: {'positive_pairs': 195386, 'negative_pairs': 195386, 'total_pairs': 390772}\n",
      "\n",
      "Validation Statistics: {'positive_pairs': 61638, 'negative_pairs': 61638, 'total_pairs': 123276}\n",
      "\n",
      "Test Statistics: {'positive_pairs': 20147, 'negative_pairs': 20147, 'total_pairs': 40294}\n"
     ]
    }
   ],
   "source": [
    "balanced_classes = True\n",
    "train_ratio = 0.8\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "dataset = KinshipDataset(train_pairs_dir, train_families_dir,test_faces_dir,test_relationship_lists_dir,test_relationship_labels_dir, train_ratio=train_ratio, validation_ratio=validation_ratio, test_ratio=test_ratio, balanced_classes = balanced_classes)\n",
    "\n",
    "train_pairs, validation_pairs, test_pairs, original_test_pairs = dataset.train_pairs, dataset.validation_pairs, dataset.test_pairs, dataset.original_test_pairs\n",
    "\n",
    "\n",
    "train_dataset = KinshipPairs(train_pairs, train_families_dir)\n",
    "validation_dataset = KinshipPairs(validation_pairs, train_families_dir)\n",
    "test_dataset = KinshipPairs(test_pairs, train_families_dir)\n",
    "original_test_dataset = KinshipPairs(original_test_pairs, test_faces_dir)\n",
    "\n",
    "train_stats = dataset.get_statistics(train_pairs)\n",
    "validation_stats = dataset.get_statistics(validation_pairs)\n",
    "test_stats = dataset.get_statistics(test_pairs)\n",
    "\n",
    "\n",
    "print(\" ############ Created Datasets with the following characteristics: ############\")\n",
    "print(f\"Balanced classes: {balanced_classes}\")\n",
    "print(f\"Train ratio: {train_ratio}\")\n",
    "print(f\"Validation ratio: {validation_ratio}\")\n",
    "print(f\"Test ratio: {test_ratio}\")\n",
    "print(\"#### Families ####\")\n",
    "families = set(dataset.train_families_data_dict.keys())\n",
    "print(f\"Number of training families: {int(len(families)*(1-validation_ratio))}\")\n",
    "print(f\"Number of validation families: {int(len(families)*validation_ratio)}\")\n",
    "print(f\"Number of test families: {int(len(families)*test_ratio)}\")\n",
    "print(\"#### Set Pairs ####\")\n",
    "print(\"\\nTrain Statistics:\", train_stats)\n",
    "print(\"\\nValidation Statistics:\", validation_stats)\n",
    "print(\"\\nTest Statistics:\", test_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f9ce0-4810-4e27-ae81-ef607ee788c3",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48581671-521c-4b7d-a0ba-752975833ebb",
   "metadata": {},
   "source": [
    "### Dence Classification Models (Face Pretrained Embedding Model Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59c687a3-949c-4510-89e6-343b0768c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kinship_model(architecture):\n",
    "    \"\"\"\n",
    "    Create a kinship model using pre-trained InceptionResnetV1 for face embeddings.\n",
    "    \n",
    "    :return: nn.Module, the kinship model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load pre-trained InceptionResnetV1 model from facenet-pytorch\n",
    "    backbone = InceptionResnetV1(pretrained='vggface2').eval()  # Using the VGGFace2 dataset weights\n",
    "    \n",
    "    # Return the model instance\n",
    "    return KinshipModel(backbone, architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "155fd90a-ddbc-40ba-8455-f5ae3806c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KinshipModel(nn.Module):\n",
    "    def __init__(self, encoder_backbone, architecture):\n",
    "        super(KinshipModel, self).__init__()\n",
    "        self.backbone = encoder_backbone  # Use the pre-trained face recognition model\n",
    "        \n",
    "        # Determine embedding dimension dynamically from the backbone\n",
    "        dummy_input = torch.randn(1, 3, 160, 160)  # Example input size, matches Facenet's expected input\n",
    "        self.embedding_dim = self.backbone(dummy_input).size(1)  # Get the embedding dimension from output size\n",
    "\n",
    "        if architecture == \"SEC\":\n",
    "            self.sequential_input_size = self.embedding_dim*2\n",
    "        elif architecture == \"SSC\":\n",
    "            self.sequential_input_size = self.embedding_dim + 2\n",
    "        elif architecture == \"SESC\":\n",
    "            self.sequential_input_size = self.embedding_dim*3 + 2\n",
    "        else:\n",
    "            raise Exception(\"Architecture must be astring from the following options: SEC, SSC, SESC\")\n",
    "\n",
    "        self.architecture = architecture\n",
    "\n",
    "        ####################### Dense Classification Layers #######################\n",
    "        # Binary classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.sequential_input_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    ################################################################## Forward Function ##################################################################\n",
    "    def forward(self, img1, img2):\n",
    "        # Extract face embeddings for both images\n",
    "        img1_embedding = self.backbone(img1)  # Shape: [batch_size, embedding_dim]\n",
    "        img2_embedding = self.backbone(img2)  # Shape: [batch_size, embedding_dim]\n",
    "\n",
    "        ############ Similarity Calculations ############\n",
    "        # Calculate cosine similarity (scalar)\n",
    "        cosine_sim = F.cosine_similarity(img1_embedding, img2_embedding, dim=1)\n",
    "        # Calculate Euclidean distance (scalar)\n",
    "        euclidean_dist = F.pairwise_distance(img1_embedding, img2_embedding, p=2)\n",
    "        # Calculate squared difference (vector of shape [batch_size, embedding_dim])\n",
    "        squared_diff = torch.pow(img1_embedding - img2_embedding, 2)\n",
    "\n",
    "        # Classifier FFNN Input\n",
    "        if self.architecture == \"SEC\":\n",
    "            classifier_input = torch.cat((img1_embedding, img2_embedding), dim=1)\n",
    "        elif self.architecture == \"SSC\":\n",
    "            classifier_input = torch.cat((squared_diff, cosine_sim.unsqueeze(1), euclidean_dist.unsqueeze(1)), dim=1)\n",
    "        elif self.architecture == \"SESC\":\n",
    "            classifier_input = torch.cat((img1_embedding, img2_embedding, squared_diff, cosine_sim.unsqueeze(1), euclidean_dist.unsqueeze(1)), dim=1)\n",
    "\n",
    "        # Feed the concatenated similarity metrics through the classifier\n",
    "        out = self.classifier(classifier_input)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe2244-bbf7-461c-b6e3-07debb75f371",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### SEC (Siamese Embedding Concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98e9876c-1438-4b5a-ba8a-9bc07ce2b837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KinshipModel(nn.Module):\n",
    "    def __init__(self, architecture):\n",
    "        super(KinshipModel, self).__init__()\n",
    "        self.backbone = architecture  # Use the pre-trained face recognition model\n",
    "        \n",
    "        # Determine embedding dimension dynamically from the backbone\n",
    "        dummy_input = torch.randn(1, 3, 160, 160)  # Example input size, matches Facenet's expected input\n",
    "        self.embedding_dim = self.backbone(dummy_input).size(1)  # Get the embedding dimension from output size\n",
    "\n",
    "        ####################### Dense Classification Layers #######################\n",
    "        # Binary classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim*2, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    ################################################################## Forward Function ##################################################################\n",
    "    def forward(self, img1, img2):\n",
    "        # Extract face embeddings for both images\n",
    "        img1_embedding = self.backbone(img1)  # Shape: [batch_size, embedding_dim]\n",
    "        img2_embedding = self.backbone(img2)  # Shape: [batch_size, embedding_dim]\n",
    "\n",
    "        # Concatenate all similarity metrics: [squared_diff, cosine_sim, euclidean_dist]\n",
    "        concatenated_embeddings = torch.cat((img1_embedding, img2_embedding), dim=1)\n",
    "\n",
    "        # Feed the concatenated similarity metrics through the classifier\n",
    "        out = self.classifier(concatenated_embeddings)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14491b-1559-4b31-b0ef-fc4fcaa51057",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### SSC (Siamese Similarity Concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf360b-cd45-43df-a49f-d8ce562c3663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KinshipModel(nn.Module):\n",
    "    def __init__(self, architecture):\n",
    "        super(KinshipModel, self).__init__()\n",
    "        self.backbone = architecture  # Use the pre-trained face recognition model\n",
    "        \n",
    "        # Determine embedding dimension dynamically from the backbone\n",
    "        dummy_input = torch.randn(1, 3, 160, 160)  # Example input size, matches Facenet's expected input\n",
    "        self.embedding_dim = self.backbone(dummy_input).size(1)  # Get the embedding dimension from output size\n",
    "\n",
    "        ####################### Dense Classification Layers #######################\n",
    "        # Binary classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim + 2, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    ################################################################## Forward Function ##################################################################\n",
    "    def forward(self, img1, img2):\n",
    "        # Extract face embeddings for both images\n",
    "        img1_embedding = self.backbone(img1)  # Shape: [batch_size, embedding_dim]\n",
    "        img2_embedding = self.backbone(img2)  # Shape: [batch_size, embedding_dim]\n",
    "\n",
    "        # Calculate cosine similarity (scalar)\n",
    "        cosine_sim = F.cosine_similarity(img1_embedding, img2_embedding, dim=1)\n",
    "\n",
    "        # Calculate Euclidean distance (scalar)\n",
    "        euclidean_dist = F.pairwise_distance(img1_embedding, img2_embedding, p=2)\n",
    "\n",
    "        # Calculate squared difference (vector of shape [batch_size, embedding_dim])\n",
    "        squared_diff = torch.pow(img1_embedding - img2_embedding, 2)\n",
    "\n",
    "        # Concatenate all similarity metrics: [squared_diff, cosine_sim, euclidean_dist]\n",
    "        similarity = torch.cat((squared_diff, cosine_sim.unsqueeze(1), euclidean_dist.unsqueeze(1)), dim=1)\n",
    "\n",
    "        # Feed the concatenated similarity metrics through the classifier\n",
    "        out = self.classifier(similarity)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518a834-e40b-450c-99a2-da96b6787c75",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### SESC (Siamese Embedding and Similarity Concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a42f292-a726-4567-954a-9914310d2d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KinshipModel(nn.Module):\n",
    "    def __init__(self, architecture):\n",
    "        super(KinshipModel, self).__init__()\n",
    "        self.backbone = architecture  # Use the pre-trained face recognition model\n",
    "        \n",
    "        # Determine embedding dimension dynamically from the backbone\n",
    "        dummy_input = torch.randn(1, 3, 160, 160)  # Example input size, matches Facenet's expected input\n",
    "        self.embedding_dim = self.backbone(dummy_input).size(1)  # Get the embedding dimension from output size\n",
    "\n",
    "        ####################### Dense Classification Layers #######################\n",
    "        # Binary classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim*3 + 2, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    ################################################################## Forward Function ##################################################################\n",
    "    def forward(self, img1, img2):\n",
    "        # Extract face embeddings for both images\n",
    "        img1_embedding = self.backbone(img1)  # Shape: [batch_size, embedding_dim]\n",
    "        img2_embedding = self.backbone(img2)  # Shape: [batch_size, embedding_dim]\n",
    "\n",
    "        # Calculate cosine similarity (scalar)\n",
    "        cosine_sim = F.cosine_similarity(img1_embedding, img2_embedding, dim=1)\n",
    "\n",
    "        # Calculate Euclidean distance (scalar)\n",
    "        euclidean_dist = F.pairwise_distance(img1_embedding, img2_embedding, p=2)\n",
    "\n",
    "        # Calculate squared difference (vector of shape [batch_size, embedding_dim])\n",
    "        squared_diff = torch.pow(img1_embedding - img2_embedding, 2)\n",
    "\n",
    "        # Concatenate all similarity metrics: [squared_diff, cosine_sim, euclidean_dist]\n",
    "        similarity = torch.cat((img1_embedding, img2_embedding, squared_diff, cosine_sim.unsqueeze(1), euclidean_dist.unsqueeze(1)), dim=1)\n",
    "\n",
    "        # Feed the concatenated similarity metrics through the classifier\n",
    "        out = self.classifier(similarity)\n",
    "        return out\n",
    "\n",
    "def create_kinship_model():\n",
    "    \"\"\"\n",
    "    Create a kinship model using pre-trained InceptionResnetV1 for face embeddings.\n",
    "    \n",
    "    :return: nn.Module, the kinship model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load pre-trained InceptionResnetV1 model from facenet-pytorch\n",
    "    backbone = InceptionResnetV1(pretrained='vggface2').eval()  # Using the VGGFace2 dataset weights\n",
    "    \n",
    "    # Return the model instance\n",
    "    return KinshipModel(backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2828483-0965-43a3-8a94-09462ee644e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test embedding dimantions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cbb9a1e-25a6-4802-8baf-0b454dd721db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([1, 512])\n",
      "Embedding dimension: 512\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "# Load the pre-trained InceptionResnetV1 model\n",
    "backbone = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# Create a dummy input tensor (batch size 1, 3 color channels, 160x160 image)\n",
    "dummy_input = torch.randn(1, 3, 160, 160)\n",
    "\n",
    "# Pass the dummy input through the backbone model\n",
    "embedding = backbone(dummy_input)\n",
    "\n",
    "# Print the embedding dimension\n",
    "print(f\"Embedding shape: {embedding.shape}\")  # Expected output: (1, embedding_dim)\n",
    "print(f\"Embedding dimension: {embedding.shape[1]}\")  # The actual dimension value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e80f2-da60-4b4d-ab26-edac20652a79",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11803a45-e8c6-4d73-b0a5-77fbad188517",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7f0d4b5-b915-41c8-83ac-e810e78ded9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, device, threshold = 0.5):\n",
    "    \"\"\"\n",
    "    This function is aimed at allowing to validate model performance by saving the true and predicted probabilities\n",
    "    to then evaluate different thresholds and metrics\n",
    "    :param model:\n",
    "    :param val_loader:\n",
    "    :param device:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    ######### Prep #########\n",
    "    model_name = model.__class__.__name__\n",
    "    model.eval()\n",
    "    all_img1_paths, all_img2_paths = [],[]\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    ######### Validation Feedforward #########\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, labels, img1_paths, img2_paths in val_loader:\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "            outputs = model(img1, img2).squeeze()\n",
    "\n",
    "            all_img1_paths.extend(img1_paths)\n",
    "            all_img2_paths.extend(img2_paths)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    ######### Save CSV #########\n",
    "    results_df = pd.DataFrame()\n",
    "    results_df['img1_path'] = all_img1_paths\n",
    "    results_df['img2_path'] = all_img2_paths\n",
    "    results_df['true'] = all_labels\n",
    "    results_df['pred'] = all_preds\n",
    "    # results_df.to_csv(f\"{model_name}_Validation_Results.csv\")\n",
    "\n",
    "    ######### Evaluate Using Threshold #########\n",
    "    binary_preds = [1 if p > threshold else 0 for p in all_preds]\n",
    "    accuracy = accuracy_score(all_labels, binary_preds)\n",
    "    roc_auc = roc_auc_score(all_labels, all_preds)  # Use raw probabilities for ROC AUC\n",
    "\n",
    "    return accuracy, roc_auc, results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365264ec-a011-4bd6-ba36-f306c7d66b7f",
   "metadata": {},
   "source": [
    "### Run Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f2f639-63b5-4b67-9827-465f1461d80c",
   "metadata": {},
   "source": [
    "#### Inception Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74207aa7-2339-42ad-a85d-ac586f538241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patience: 3\n",
      "Max Epochs: 20\n",
      "Device Used: cuda\n",
      "\n",
      "Creating Data Loaders...\n",
      "Finished\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "############################################################# SEC #############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 3053/3053 [33:17<00:00,  1.53it/s, loss=0.3636] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 1 ####\n",
      "Average Training Loss: 0.3636\n",
      "Validation Accuracy: 0.6664\n",
      "Validation ROC AUC: 0.7410\n",
      "Validation AUC improved. Best model saved.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3053/3053 [33:03<00:00,  1.54it/s, loss=0.1690] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 2 ####\n",
      "Average Training Loss: 0.1690\n",
      "Validation Accuracy: 0.6985\n",
      "Validation ROC AUC: 0.7931\n",
      "Validation AUC improved. Best model saved.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3053/3053 [31:22<00:00,  1.62it/s, loss=0.1054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 3 ####\n",
      "Average Training Loss: 0.1054\n",
      "Validation Accuracy: 0.6706\n",
      "Validation ROC AUC: 0.7713\n",
      "No improvement. Patience Counter: 1/3\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 3053/3053 [31:18<00:00,  1.63it/s, loss=0.0757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 4 ####\n",
      "Average Training Loss: 0.0757\n",
      "Validation Accuracy: 0.6793\n",
      "Validation ROC AUC: 0.7815\n",
      "No improvement. Patience Counter: 2/3\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 3053/3053 [30:52<00:00,  1.65it/s, loss=0.0588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 5 ####\n",
      "Average Training Loss: 0.0588\n",
      "Validation Accuracy: 0.6663\n",
      "Validation ROC AUC: 0.7785\n",
      "No improvement. Patience Counter: 3/3\n",
      "Early stopping triggered.\n",
      "\n",
      "\n",
      "Training Complete.\n",
      "\n",
      "Testing on the Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 315/315 [02:37<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.7887\n",
      "Test results saved to best model results/test csv results/SEC_Test_Results.csv\n",
      "Metrics saved to best model results/metric results/SEC_metrics.txt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "############################################################# SSC #############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 3053/3053 [31:26<00:00,  1.62it/s, loss=0.2869] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 1 ####\n",
      "Average Training Loss: 0.2869\n",
      "Validation Accuracy: 0.7391\n",
      "Validation ROC AUC: 0.8046\n",
      "Validation AUC improved. Best model saved.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3053/3053 [33:15<00:00,  1.53it/s, loss=0.0941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 2 ####\n",
      "Average Training Loss: 0.0941\n",
      "Validation Accuracy: 0.7239\n",
      "Validation ROC AUC: 0.8013\n",
      "No improvement. Patience Counter: 1/3\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3053/3053 [31:48<00:00,  1.60it/s, loss=0.0488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 3 ####\n",
      "Average Training Loss: 0.0488\n",
      "Validation Accuracy: 0.7121\n",
      "Validation ROC AUC: 0.8008\n",
      "No improvement. Patience Counter: 2/3\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 3053/3053 [32:59<00:00,  1.54it/s, loss=0.0313]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 4 ####\n",
      "Average Training Loss: 0.0313\n",
      "Validation Accuracy: 0.7157\n",
      "Validation ROC AUC: 0.8006\n",
      "No improvement. Patience Counter: 3/3\n",
      "Early stopping triggered.\n",
      "\n",
      "\n",
      "Training Complete.\n",
      "\n",
      "Testing on the Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 315/315 [02:45<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.7968\n",
      "Test results saved to best model results/test csv results/SSC_Test_Results.csv\n",
      "Metrics saved to best model results/metric results/SSC_metrics.txt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "############################################################# SESC #############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 3053/3053 [30:52<00:00,  1.65it/s, loss=0.2698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 1 ####\n",
      "Average Training Loss: 0.2698\n",
      "Validation Accuracy: 0.7333\n",
      "Validation ROC AUC: 0.8169\n",
      "Validation AUC improved. Best model saved.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3053/3053 [31:28<00:00,  1.62it/s, loss=0.0909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 2 ####\n",
      "Average Training Loss: 0.0909\n",
      "Validation Accuracy: 0.7186\n",
      "Validation ROC AUC: 0.8144\n",
      "No improvement. Patience Counter: 1/3\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3053/3053 [32:07<00:00,  1.58it/s, loss=0.0486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 3 ####\n",
      "Average Training Loss: 0.0486\n",
      "Validation Accuracy: 0.7200\n",
      "Validation ROC AUC: 0.8296\n",
      "Validation AUC improved. Best model saved.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 3053/3053 [32:45<00:00,  1.55it/s, loss=0.0318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 4 ####\n",
      "Average Training Loss: 0.0318\n",
      "Validation Accuracy: 0.7243\n",
      "Validation ROC AUC: 0.8349\n",
      "Validation AUC improved. Best model saved.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 3053/3053 [31:13<00:00,  1.63it/s, loss=0.0234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 5 ####\n",
      "Average Training Loss: 0.0234\n",
      "Validation Accuracy: 0.7264\n",
      "Validation ROC AUC: 0.8344\n",
      "No improvement. Patience Counter: 1/3\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 3053/3053 [31:35<00:00,  1.61it/s, loss=0.0189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 6 ####\n",
      "Average Training Loss: 0.0189\n",
      "Validation Accuracy: 0.7143\n",
      "Validation ROC AUC: 0.8296\n",
      "No improvement. Patience Counter: 2/3\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 3053/3053 [31:19<00:00,  1.62it/s, loss=0.0157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 7 ####\n",
      "Average Training Loss: 0.0157\n",
      "Validation Accuracy: 0.7249\n",
      "Validation ROC AUC: 0.8408\n",
      "Validation AUC improved. Best model saved.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 3053/3053 [31:40<00:00,  1.61it/s, loss=0.0134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 8 ####\n",
      "Average Training Loss: 0.0134\n",
      "Validation Accuracy: 0.7260\n",
      "Validation ROC AUC: 0.8423\n",
      "Validation AUC improved. Best model saved.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 3053/3053 [30:10<00:00,  1.69it/s, loss=0.0120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 9 ####\n",
      "Average Training Loss: 0.0120\n",
      "Validation Accuracy: 0.7307\n",
      "Validation ROC AUC: 0.8464\n",
      "Validation AUC improved. Best model saved.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 3053/3053 [30:00<00:00,  1.70it/s, loss=0.0105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 10 ####\n",
      "Average Training Loss: 0.0105\n",
      "Validation Accuracy: 0.7269\n",
      "Validation ROC AUC: 0.8485\n",
      "Validation AUC improved. Best model saved.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 3053/3053 [30:15<00:00,  1.68it/s, loss=0.0095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 11 ####\n",
      "Average Training Loss: 0.0095\n",
      "Validation Accuracy: 0.6962\n",
      "Validation ROC AUC: 0.8318\n",
      "No improvement. Patience Counter: 1/3\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 3053/3053 [30:01<00:00,  1.70it/s, loss=0.0089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 12 ####\n",
      "Average Training Loss: 0.0089\n",
      "Validation Accuracy: 0.7176\n",
      "Validation ROC AUC: 0.8449\n",
      "No improvement. Patience Counter: 2/3\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 3053/3053 [30:03<00:00,  1.69it/s, loss=0.0079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 13 ####\n",
      "Average Training Loss: 0.0079\n",
      "Validation Accuracy: 0.7241\n",
      "Validation ROC AUC: 0.8539\n",
      "Validation AUC improved. Best model saved.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 3053/3053 [30:27<00:00,  1.67it/s, loss=0.0075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 14 ####\n",
      "Average Training Loss: 0.0075\n",
      "Validation Accuracy: 0.7290\n",
      "Validation ROC AUC: 0.8553\n",
      "Validation AUC improved. Best model saved.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 3053/3053 [30:09<00:00,  1.69it/s, loss=0.0072]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 15 ####\n",
      "Average Training Loss: 0.0072\n",
      "Validation Accuracy: 0.7046\n",
      "Validation ROC AUC: 0.8411\n",
      "No improvement. Patience Counter: 1/3\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 3053/3053 [30:45<00:00,  1.65it/s, loss=0.0065] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Epoch 16 ####\n",
      "Average Training Loss: 0.0065\n",
      "Validation Accuracy: 0.6933\n",
      "Validation ROC AUC: 0.8412\n",
      "No improvement. Patience Counter: 2/3\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17:  52%|█████▏    | 1581/3053 [15:30<14:24,  1.70it/s, loss=0.0063]"
     ]
    }
   ],
   "source": [
    "#################################################################################### Init & Prep ####################################################################################\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Parameters\n",
    "patience = 3\n",
    "max_epochs = 20\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "print(f\"Patience: {patience}\")\n",
    "print(f\"Max Epochs: {max_epochs}\")\n",
    "print(f\"Device Used: {device}\")\n",
    "\n",
    "\n",
    "#################################################################################### Data Loaders ####################################################################################\n",
    "\n",
    "IS_TEST = False\n",
    "\n",
    "if IS_TEST:\n",
    "    # Subsample the datasets for quick testing\n",
    "    sample_size = 1000  # Adjust this size as needed\n",
    "    train_dataset_small = torch.utils.data.Subset(train_dataset, range(min(len(train_dataset), sample_size)))\n",
    "    validation_dataset_small = torch.utils.data.Subset(validation_dataset,range(min(len(validation_dataset), sample_size)))\n",
    "    test_dataset_small = torch.utils.data.Subset(test_dataset, range(min(len(test_dataset), sample_size)))\n",
    "\n",
    "    # Create small data loaders\n",
    "    train_loader = DataLoader(train_dataset_small, batch_size=128)  # Smaller batch size for quick tests\n",
    "    validation_loader = DataLoader(validation_dataset_small, batch_size=128)\n",
    "    test_loader = DataLoader(test_dataset_small, batch_size=128)\n",
    "\n",
    "else:\n",
    "    # Create data loaders\n",
    "    print(\"\\nCreating Data Loaders...\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=128)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "    print(\"Finished\")\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################### Run Experiments ####################################################################################\n",
    "\n",
    "# Iterate over each architecture\n",
    "for base_architecture in ['SEC','SSC','SESC']:\n",
    "\n",
    "    print(f\"\\n\\n\\n\\n############################################################# {base_architecture} #############################################################\")\n",
    "    \n",
    "    # Model\n",
    "    model = create_kinship_model(base_architecture)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Initialization\n",
    "    best_validation_auc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_epoch = -1\n",
    "\n",
    "    # Training using epochs\n",
    "    for epoch in range(max_epochs):  # Set maximum epochs\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "    \n",
    "        ################################################# Training #################################################\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\") # Initialize the progress bar for the epoch\n",
    "\n",
    "        # Training Batches\n",
    "        for batch_idx, (img1, img2, labels, img1_path, imag2_path) in progress_bar:\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(img1, img2).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            \n",
    "            progress_bar.set_postfix(loss=f\"{total_loss / (batch_idx + 1):.4f}\") # Update the progress bar with the current loss\n",
    "            \n",
    "\n",
    "        ################################################# Validation #################################################\n",
    "        # Evaluate the model on the validation set\n",
    "        model.eval()\n",
    "        val_accuracy, val_auc, _ = evaluate_model(model, validation_loader, device)\n",
    "\n",
    "        \n",
    "        print(f\"\\n#### Epoch {epoch + 1} ####\")\n",
    "        print(f\"Average Training Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Validation ROC AUC: {val_auc:.4f}\")\n",
    "\n",
    "        ################################################# Update Best Validation Score/Model #################################################\n",
    "        # Check if validation performance has improved\n",
    "        if val_auc > best_validation_auc:\n",
    "            best_validation_auc = val_auc\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            # Save the best model based on validation performance\n",
    "            Path(\"best models\").mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"best models/best_kinship_{base_architecture}_model.pth\")\n",
    "            print(\"Validation AUC improved. Best model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience Counter: {patience_counter}/{patience}\")\n",
    "    \n",
    "        # Early stopping condition\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    \n",
    "        print(\"\\n\\n\")\n",
    "    \n",
    "    print(f\"\\n\\nTraining Complete.\")\n",
    "    \n",
    "    \n",
    "    ################################################# Test #################################################\n",
    "    # Test the best model on the test set\n",
    "    print(\"\\nTesting on the Test Set...\")\n",
    "    model.load_state_dict(torch.load(f\"best models/best_kinship_{base_architecture}_model.pth\", weights_only=True))\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, labels, img1_path, img2_path in tqdm(test_loader, desc=\"Testing\"):\n",
    "            img1, img2 = img1.to(device), img2.to(device)\n",
    "            raw_outputs = model(img1, img2).squeeze()  # Raw predictions\n",
    "            results.extend([\n",
    "                {\"label\": label.item(), \"predicted\": raw_output.item(),\n",
    "                 \"image 1 full path\": img1_path[i], \"image 2 full path\": img2_path[i]}\n",
    "                for i, (label, raw_output) in enumerate(zip(labels, raw_outputs))\n",
    "            ])\n",
    "\n",
    "    # Calculate AUC for the test set\n",
    "    y_true = [result[\"label\"] for result in results]  # True labels\n",
    "    y_pred = [result[\"predicted\"] for result in results]  # Predicted probabilities\n",
    "    test_auc = roc_auc_score(y_true, y_pred)\n",
    "    print(f\"Test ROC AUC: {test_auc:.4f}\")\n",
    "\n",
    "    ################################################# Saving Results #################################################\n",
    "\n",
    "    # Create directories\n",
    "    base_results_dir = Path(\"best model results\")\n",
    "    metrics_dir = base_results_dir / \"metric results\"\n",
    "    csv_dir = base_results_dir / \"test csv results\"\n",
    "    metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "    csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save metrics to a text file\n",
    "    metrics_file = metrics_dir / f\"{base_architecture}_metrics.txt\"\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        f.write(f\"Architecture: {base_architecture}\\n\")\n",
    "        f.write(f\"Best Validation AUC: {best_validation_auc:.4f}\\n\")\n",
    "        f.write(f\"Best Epoch: {best_epoch + 1}\\n\")  # Epochs are 0-indexed in code\n",
    "        f.write(f\"Test AUC: {test_auc:.4f}\\n\")\n",
    "    \n",
    "\n",
    "    # Save the results as a CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    test_csv_path = csv_dir / f\"{base_architecture}_Test_Results.csv\"\n",
    "    results_df.to_csv(test_csv_path, index=False)\n",
    "    print(f\"Test results saved to {test_csv_path}\")\n",
    "    print(f\"Metrics saved to {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47330da4-0566-43fa-ac32-28b96fd614c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Old Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a47176a-7a48-4f1c-a431-b9cc121a48cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patience: 3\n",
      "Max Epochs: 20\n",
      "Device Used: cuda\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "############################################################# vggface2 #############################################################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b968470c6824aedaf315a6f45b6880a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/107M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/16 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KinshipPairs' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Initialize the progress bar for the epoch\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Training Batches\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (img1, img2, labels, _, _) \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m     68\u001b[0m     img1, img2, labels \u001b[38;5;241m=\u001b[39m img1\u001b[38;5;241m.\u001b[39mto(device), img2\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     69\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.conda/envs/kinship_env/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/kinship_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/kinship_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/kinship_env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/kinship_env/lib/python3.12/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/sise/home/dzega/projects/kinship_detection_project/Code/image_kinship_identification/kinship_image_data_class.py:57\u001b[0m, in \u001b[0;36mKinshipPairs.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     54\u001b[0m img1 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(full_img1_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m img2 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(full_img2_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     58\u001b[0m     img1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img1)\n\u001b[1;32m     59\u001b[0m     img2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img2)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KinshipPairs' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "#################################################################################### Init & Prep ####################################################################################\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Parameters\n",
    "patience = 3\n",
    "max_epochs = 20\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "print(f\"Patience: {patience}\")\n",
    "print(f\"Max Epochs: {max_epochs}\")\n",
    "print(f\"Device Used: {device}\")\n",
    "\n",
    "\n",
    "#################################################################################### Data Loaders ####################################################################################\n",
    "\n",
    "IS_TEST = True\n",
    "\n",
    "if IS_TEST:\n",
    "    # Subsample the datasets for quick testing\n",
    "    sample_size = 1000  # Adjust this size as needed\n",
    "    train_dataset_small = torch.utils.data.Subset(train_dataset, range(min(len(train_dataset), sample_size)))\n",
    "    validation_dataset_small = torch.utils.data.Subset(validation_dataset,range(min(len(validation_dataset), sample_size)))\n",
    "    test_dataset_small = torch.utils.data.Subset(test_dataset, range(min(len(test_dataset), sample_size)))\n",
    "\n",
    "    # Create small data loaders\n",
    "    train_loader = DataLoader(train_dataset_small, batch_size=64)  # Smaller batch size for quick tests\n",
    "    validation_loader = DataLoader(validation_dataset_small, batch_size=64)\n",
    "    test_loader = DataLoader(test_dataset_small, batch_size=64)\n",
    "\n",
    "else:\n",
    "    # Create data loaders\n",
    "    print(\"\\nCreating Data Loaders...\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=128)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "    print(\"Finished\")\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################### Run Experiments ####################################################################################\n",
    "\n",
    "# Iterate over each architecture\n",
    "for base_architecture in [\"vggface2\"]: # \"vgg16\",\"vgg19\",\"resnet50\",\"resnet152\"\n",
    "\n",
    "    print(f\"\\n\\n\\n\\n############################################################# {base_architecture} #############################################################\")\n",
    "    \n",
    "    # Model\n",
    "    model = create_kinship_model(base_architecture, embedding_dim=256)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Initialization\n",
    "    best_validation_auc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_epoch = -1\n",
    "\n",
    "    # Training using epochs\n",
    "    for epoch in range(max_epochs):  # Set maximum epochs\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "    \n",
    "        ################################################# Training #################################################\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}\") # Initialize the progress bar for the epoch\n",
    "\n",
    "        # Training Batches\n",
    "        for batch_idx, (img1, img2, labels, img1_path, imag2_path) in progress_bar:\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(img1, img2).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            \n",
    "            progress_bar.set_postfix(loss=f\"{total_loss / (batch_idx + 1):.4f}\") # Update the progress bar with the current loss\n",
    "            \n",
    "\n",
    "        ################################################# Validation #################################################\n",
    "        # Evaluate the model on the validation set\n",
    "        model.eval()\n",
    "        val_accuracy, val_auc, _ = evaluate_model(model, validation_loader, device)\n",
    "\n",
    "        \n",
    "        print(f\"\\n#### Epoch {epoch + 1} ####\")\n",
    "        print(f\"Average Training Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Validation ROC AUC: {val_auc:.4f}\")\n",
    "\n",
    "        ################################################# Update Best Validation Score/Model #################################################\n",
    "        # Check if validation performance has improved\n",
    "        if val_auc > best_validation_auc:\n",
    "            best_validation_auc = val_auc\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            # Save the best model based on validation performance\n",
    "            Path(\"best models\").mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"best models/best_kinship_{base_architecture}_model.pth\")\n",
    "            print(\"Validation AUC improved. Best model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience Counter: {patience_counter}/{patience}\")\n",
    "    \n",
    "        # Early stopping condition\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    \n",
    "        print(\"\\n\\n\")\n",
    "    \n",
    "    print(f\"\\n\\nTraining Complete.\")\n",
    "    \n",
    "    \n",
    "    ################################################# Test #################################################\n",
    "    # Test the best model on the test set\n",
    "    print(\"\\nTesting on the Test Set...\")\n",
    "    model.load_state_dict(torch.load(f\"best models/best_kinship_{base_architecture}_model.pth\", weights_only=True))\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, labels, img1_path, img2_path in tqdm(test_loader, desc=\"Testing\"):\n",
    "            img1, img2 = img1.to(device), img2.to(device)\n",
    "            raw_outputs = model(img1, img2).squeeze()  # Raw predictions\n",
    "            results.extend([\n",
    "                {\"label\": label.item(), \"predicted\": raw_output.item(),\n",
    "                 \"image 1 full path\": img1_path[i], \"image 2 full path\": img2_path[i]}\n",
    "                for i, (label, raw_output) in enumerate(zip(labels, raw_outputs))\n",
    "            ])\n",
    "\n",
    "    # Calculate AUC for the test set\n",
    "    y_true = [result[\"label\"] for result in results]  # True labels\n",
    "    y_pred = [result[\"predicted\"] for result in results]  # Predicted probabilities\n",
    "    test_auc = roc_auc_score(y_true, y_pred)\n",
    "    print(f\"Test ROC AUC: {test_auc:.4f}\")\n",
    "\n",
    "    ################################################# Saving Results #################################################\n",
    "\n",
    "    # Create directories\n",
    "    base_results_dir = Path(\"best model results\")\n",
    "    metrics_dir = base_results_dir / \"metric results\"\n",
    "    csv_dir = base_results_dir / \"test csv results\"\n",
    "    metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "    csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save metrics to a text file\n",
    "    metrics_file = metrics_dir / f\"{base_architecture}_metrics.txt\"\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        f.write(f\"Architecture: {base_architecture}\\n\")\n",
    "        f.write(f\"Best Validation AUC: {best_validation_auc:.4f}\\n\")\n",
    "        f.write(f\"Best Epoch: {best_epoch + 1}\\n\")  # Epochs are 0-indexed in code\n",
    "        f.write(f\"Test AUC: {test_auc:.4f}\\n\")\n",
    "    \n",
    "\n",
    "    # Save the results as a CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    test_csv_path = csv_dir / f\"{base_architecture}_Test_Results.csv\"\n",
    "    results_df.to_csv(test_csv_path, index=False)\n",
    "    print(f\"Test results saved to {test_csv_path}\")\n",
    "    print(f\"Metrics saved to {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08efa3de-691a-420d-ae31-649453d37cbb",
   "metadata": {},
   "source": [
    "# Other"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepface_env)",
   "language": "python",
   "name": "deepface_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
